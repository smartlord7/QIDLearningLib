<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>QIDLearningLib.metrics.causality API documentation</title>
<meta name="description" content="QIDLearningLib …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>QIDLearningLib.metrics.causality</code></h1>
</header>
<section id="section-intro">
<p>QIDLearningLib</p>
<p>Library Description (QIDLearningLib):
QIDLearningLib is a Python library designed to provide a comprehensive set of metrics for quasi-identification recognition processes.
The library encompasses metrics for assessing data privacy, data utility, and the performance of quasi-identification recognition algorithms.</p>
<p>Module Description (metrics.causality):
This module in QIDLearningLib includes functions to calculate various causality metrics in order to study the causal effect
between the assumed quasi identifiers and the remaining attributes.
These metrics measure aspects such as data separation, distinction.</p>
<p>Year: 2023/2024
Institution: University of Coimbra
Department: Department of Informatics Engineering
Program: Master's in Informatics Engineering - Intelligent Systems
Author: Sancho Amaral Simões
Student No: 2019217590
Emails: sanchoamaralsimoes@gmail.com (Personal)| uc2019217590@student.uc.pt | sanchosimoes@student.dei.uc.pt
Version: v0.01</p>
<p>License:
This open-source software is released under the terms of the GNU General Public License, version 3 (GPL-3.0).
For more details, see <a href="https://www.gnu.org/licenses/gpl-3.0.html">https://www.gnu.org/licenses/gpl-3.0.html</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
QIDLearningLib

Library Description (QIDLearningLib):
QIDLearningLib is a Python library designed to provide a comprehensive set of metrics for quasi-identification recognition processes.
The library encompasses metrics for assessing data privacy, data utility, and the performance of quasi-identification recognition algorithms.

Module Description (metrics.causality):
This module in QIDLearningLib includes functions to calculate various causality metrics in order to study the causal effect
between the assumed quasi identifiers and the remaining attributes.
These metrics measure aspects such as data separation, distinction.

Year: 2023/2024
Institution: University of Coimbra
Department: Department of Informatics Engineering
Program: Master&#39;s in Informatics Engineering - Intelligent Systems
Author: Sancho Amaral Simões
Student No: 2019217590
Emails: sanchoamaralsimoes@gmail.com (Personal)| uc2019217590@student.uc.pt | sanchosimoes@student.dei.uc.pt
Version: v0.01

License:
This open-source software is released under the terms of the GNU General Public License, version 3 (GPL-3.0).
For more details, see https://www.gnu.org/licenses/gpl-3.0.html

&#34;&#34;&#34;

import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

from QIDLearningLib.structure.grouped_metric import GroupedMetric
from QIDLearningLib.util.data import encode_categorical
from QIDLearningLib.util.stats import t_test, ks_test


def covariate_shift(df, quasi_identifiers, treatment_col, treatment_value):
    &#34;&#34;&#34;
    Calculate covariate shift metric for a given DataFrame and attributes.

    Synopse:
    Covariate shift metric measures the difference in distribution between treated and control groups based on specified attributes.

    Details:
    The metric is computed by comparing the distribution of attribute values between treated and control groups using the Kolmogorov-Smirnov statistic. Additionally, it considers the overall distribution difference across the entire DataFrame.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - quasi_identifiers (list): List of column names representing the quasi-identifiers.
    - treatment_col (str): The column name representing the treatment indicator.
    - treatment_value: The value indicating the treated group.

    Return:
    GroupedMetric: An object containing covariate shift metric values and corresponding group labels.

    Example:
    &gt;&gt;&gt; df = pd.DataFrame({&#39;A&#39;: [1, 2, 1, 2], &#39;B&#39;: [&#39;X&#39;, &#39;Y&#39;, &#39;X&#39;, &#39;Y&#39;], &#39;Treatment&#39;: [1, 0, 1, 0]})
    &gt;&gt;&gt; metric_result = covariate_shift(df, [&#39;A&#39;, &#39;B&#39;], &#39;Treatment&#39;, 1)
    &gt;&gt;&gt; print(metric_result.values)
    [0.1, 0.2, ...]  # Covariate shift metric values
    &gt;&gt;&gt; print(metric_result.group_labels)
    [&#39;Group 1&#39;, &#39;Group 2&#39;, ...]  # Corresponding group labels

    &#34;&#34;&#34;
    treated = df[df[treatment_col] == treatment_value]
    control = df[df[treatment_col] != treatment_value]

    overall_distribution = df[quasi_identifiers].value_counts(normalize=True)

    grouped_treated = treated.groupby(quasi_identifiers)
    grouped_control = control.groupby(quasi_identifiers)

    shift_metrics = []
    group_labels = []

    # Flatten data and encode categorical columns
    df_flat_encoded = encode_categorical(df[quasi_identifiers].reset_index(drop=True), quasi_identifiers)

    for group_name, group_df_treated in grouped_treated:
        # Check if the control group also has data for the same group_name
        if group_name in grouped_control.groups:
            group_df_control = grouped_control.get_group(group_name)

            # Flatten data and encode categorical columns in treated and control groups
            group_df_treated_flat_encoded = encode_categorical(group_df_treated.reset_index(drop=True), quasi_identifiers)
            group_df_control_flat_encoded = encode_categorical(group_df_control.reset_index(drop=True), quasi_identifiers)

            # Check if the groups have data
            if not group_df_treated_flat_encoded.empty and not group_df_control_flat_encoded.empty:
                # Calculate overall distribution difference
                overall_diff = np.sum(
                    np.abs(overall_distribution - group_df_treated_flat_encoded.value_counts(normalize=True, sort=False)))

                # Calculate covariate shift metric
                ks_statistic, _ = ks_test(
                    df_flat_encoded.loc[group_df_treated_flat_encoded.index].values.flatten(),
                    df_flat_encoded.loc[group_df_control_flat_encoded.index].values.flatten()
                )

                shift_metrics.append(ks_statistic + overall_diff)
                group_labels.append(group_name)

    values = np.array(shift_metrics)
    return GroupedMetric(values, group_labels, name=&#39;Covariate Shift&#39;)


def balance_test(df, quasi_identifiers, treatment_col, treatment_value):
    &#34;&#34;&#34;
    Perform balance test for a given DataFrame and attributes.

    Synopse:
    Balance test measures the balance between treated and control groups based on specified quasi-identifiers.

    Details:
    The metric is computed by applying a t-test to each group defined by the quasi-identifiers, comparing the distribution of values between treated and control groups.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - quasi_identifiers (list): List of column names representing the quasi-identifiers.
    - treatment_col (str): The column name representing the treatment indicator.
    - treatment_value: The value indicating the treated group.

    Return:
    GroupedMetric: An object containing balance test metric values and corresponding group labels.

    Example:
    &gt;&gt;&gt; df = pd.DataFrame({&#39;A&#39;: [1, 2, 1, 2], &#39;B&#39;: [&#39;X&#39;, &#39;Y&#39;, &#39;X&#39;, &#39;Y&#39;], &#39;Treatment&#39;: [1, 0, 1, 0]})
    &gt;&gt;&gt; metric_result = balance_test(df, [&#39;A&#39;, &#39;B&#39;], &#39;Treatment&#39;, 1)
    &gt;&gt;&gt; print(metric_result.values)
    [0.1, 0.2, ...]  # Balance test metric values
    &gt;&gt;&gt; print(metric_result.group_labels)
    [&#39;Group 1&#39;, &#39;Group 2&#39;, ...]  # Corresponding group labels

    &#34;&#34;&#34;
    treated = df[df[treatment_col] == treatment_value]
    control = df[df[treatment_col] != treatment_value]

    grouped_treated = treated.groupby(quasi_identifiers)
    grouped_control = control.groupby(quasi_identifiers)

    balance_metrics = []
    group_labels = []

    # Flatten data and encode categorical columns
    df_flat_encoded = encode_categorical(df[quasi_identifiers].reset_index(drop=True), quasi_identifiers)

    for group_name, group_df_treated in grouped_treated:
        # Check if the control group also has data for the same group_name
        if group_name in grouped_control.groups:
            group_df_control = grouped_control.get_group(group_name)

            # Flatten data and encode categorical columns in treated and control groups
            group_df_treated_flat_encoded = encode_categorical(group_df_treated.reset_index(drop=True), quasi_identifiers)
            group_df_control_flat_encoded = encode_categorical(group_df_control.reset_index(drop=True), quasi_identifiers)

            # Check if the groups have data
            if not group_df_treated_flat_encoded.empty and not group_df_control_flat_encoded.empty:
                # Calculate balance metric
                t_statistic, _ = t_test(
                    df_flat_encoded.loc[group_df_treated_flat_encoded.index].values.flatten(),
                    df_flat_encoded.loc[group_df_control_flat_encoded.index].values.flatten()
                )

                balance_metrics.append(t_statistic)
                group_labels.append(group_name)

    values = np.array(balance_metrics)

    return GroupedMetric(values, group_labels, name=&#39;Balance Test&#39;)


def propensity_score_overlap(df, quasi_identifiers, treatment_col, treatment_value):
    &#34;&#34;&#34;
    Calculate propensity score overlap metric for a given DataFrame and attributes.

    Synopse:
    Propensity score overlap metric measures the degree of overlap in propensity scores between treated and control groups based on specified quasi-identifiers.

    Details:
    The metric is computed by fitting a logistic regression model to predict the treatment indicator based on quasi-identifiers. Propensity scores are then extracted, and the mean overlap is calculated.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - quasi_identifiers (list): List of column names representing the quasi-identifiers.
    - treatment_col (str): The column name representing the treatment indicator.
    - treatment_value: The value indicating the treated group.

    Return:
    GroupedMetric: An object containing propensity score overlap metric values and corresponding group labels.

    Example:
    &gt;&gt;&gt; df = pd.DataFrame({&#39;A&#39;: [1, 2, 1, 2], &#39;B&#39;: [&#39;X&#39;, &#39;Y&#39;, &#39;X&#39;, &#39;Y&#39;], &#39;Treatment&#39;: [1, 0, 1, 0]})
    &gt;&gt;&gt; metric_result = propensity_score_overlap(df, [&#39;A&#39;, &#39;B&#39;], &#39;Treatment&#39;, 1)
    &gt;&gt;&gt; print(metric_result.values)
    [0.1, 0.2, ...]  # Propensity score overlap metric values
    &gt;&gt;&gt; print(metric_result.group_labels)
    [&#39;Treatment 1&#39;, &#39;Treatment 2&#39;, ...]  # Corresponding group labels

    &#34;&#34;&#34;
    X = df[quasi_identifiers]
    y = df[treatment_col]

    # Identify categorical columns
    categorical_cols = X.select_dtypes(include=[&#39;object&#39;]).columns
    numerical_cols = X.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns

    # Create a ColumnTransformer to apply one-hot encoding to categorical columns
    preprocessor = ColumnTransformer(
        transformers=[
            (&#39;cat&#39;, OneHotEncoder(), categorical_cols),
            (&#39;num&#39;, &#39;passthrough&#39;, numerical_cols)
        ],
        remainder=&#39;drop&#39;
    )

    # Create a pipeline with the preprocessor and logistic regression model
    model = Pipeline(steps=[
        (&#39;preprocessor&#39;, preprocessor),
        (&#39;classifier&#39;, LogisticRegression())
    ])

    # Fit the entire pipeline
    model.fit(X, y)

    # Transform the data
    X_encoded = model.named_steps[&#39;preprocessor&#39;].transform(X)

    # Extract propensity scores for all treated samples against all control samples
    propensity_scores_treated = model.named_steps[&#39;classifier&#39;].predict_proba(X_encoded[y == treatment_value])[:, 1]
    propensity_scores_control = model.named_steps[&#39;classifier&#39;].predict_proba(X_encoded[y != treatment_value])[:, 1]

    # Calculate the mean propensity score overlap for each treated sample
    overlap_metrics = []

    for treated_score in propensity_scores_treated:
        overlap_metric = np.mean(np.abs(treated_score - propensity_scores_control))
        overlap_metrics.append(overlap_metric)

    # Use treatment labels as group labels
    group_labels = [f&#39;Treatment {i+1}&#39; for i in range(len(overlap_metrics))]

    values = np.array(overlap_metrics)

    return GroupedMetric(values, group_labels=group_labels, name=&#39;Propensity Score Overlap&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="QIDLearningLib.metrics.causality.balance_test"><code class="name flex">
<span>def <span class="ident">balance_test</span></span>(<span>df, quasi_identifiers, treatment_col, treatment_value)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform balance test for a given DataFrame and attributes.</p>
<p>Synopse:
Balance test measures the balance between treated and control groups based on specified quasi-identifiers.</p>
<p>Details:
The metric is computed by applying a t-test to each group defined by the quasi-identifiers, comparing the distribution of values between treated and control groups.</p>
<p>Parameters:
- df (pd.DataFrame): The input DataFrame.
- quasi_identifiers (list): List of column names representing the quasi-identifiers.
- treatment_col (str): The column name representing the treatment indicator.
- treatment_value: The value indicating the treated group.</p>
<p>Return:
GroupedMetric: An object containing balance test metric values and corresponding group labels.</p>
<p>Example:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 1, 2], 'B': ['X', 'Y', 'X', 'Y'], 'Treatment': [1, 0, 1, 0]})
&gt;&gt;&gt; metric_result = balance_test(df, ['A', 'B'], 'Treatment', 1)
&gt;&gt;&gt; print(metric_result.values)
[0.1, 0.2, ...]  # Balance test metric values
&gt;&gt;&gt; print(metric_result.group_labels)
['Group 1', 'Group 2', ...]  # Corresponding group labels
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def balance_test(df, quasi_identifiers, treatment_col, treatment_value):
    &#34;&#34;&#34;
    Perform balance test for a given DataFrame and attributes.

    Synopse:
    Balance test measures the balance between treated and control groups based on specified quasi-identifiers.

    Details:
    The metric is computed by applying a t-test to each group defined by the quasi-identifiers, comparing the distribution of values between treated and control groups.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - quasi_identifiers (list): List of column names representing the quasi-identifiers.
    - treatment_col (str): The column name representing the treatment indicator.
    - treatment_value: The value indicating the treated group.

    Return:
    GroupedMetric: An object containing balance test metric values and corresponding group labels.

    Example:
    &gt;&gt;&gt; df = pd.DataFrame({&#39;A&#39;: [1, 2, 1, 2], &#39;B&#39;: [&#39;X&#39;, &#39;Y&#39;, &#39;X&#39;, &#39;Y&#39;], &#39;Treatment&#39;: [1, 0, 1, 0]})
    &gt;&gt;&gt; metric_result = balance_test(df, [&#39;A&#39;, &#39;B&#39;], &#39;Treatment&#39;, 1)
    &gt;&gt;&gt; print(metric_result.values)
    [0.1, 0.2, ...]  # Balance test metric values
    &gt;&gt;&gt; print(metric_result.group_labels)
    [&#39;Group 1&#39;, &#39;Group 2&#39;, ...]  # Corresponding group labels

    &#34;&#34;&#34;
    treated = df[df[treatment_col] == treatment_value]
    control = df[df[treatment_col] != treatment_value]

    grouped_treated = treated.groupby(quasi_identifiers)
    grouped_control = control.groupby(quasi_identifiers)

    balance_metrics = []
    group_labels = []

    # Flatten data and encode categorical columns
    df_flat_encoded = encode_categorical(df[quasi_identifiers].reset_index(drop=True), quasi_identifiers)

    for group_name, group_df_treated in grouped_treated:
        # Check if the control group also has data for the same group_name
        if group_name in grouped_control.groups:
            group_df_control = grouped_control.get_group(group_name)

            # Flatten data and encode categorical columns in treated and control groups
            group_df_treated_flat_encoded = encode_categorical(group_df_treated.reset_index(drop=True), quasi_identifiers)
            group_df_control_flat_encoded = encode_categorical(group_df_control.reset_index(drop=True), quasi_identifiers)

            # Check if the groups have data
            if not group_df_treated_flat_encoded.empty and not group_df_control_flat_encoded.empty:
                # Calculate balance metric
                t_statistic, _ = t_test(
                    df_flat_encoded.loc[group_df_treated_flat_encoded.index].values.flatten(),
                    df_flat_encoded.loc[group_df_control_flat_encoded.index].values.flatten()
                )

                balance_metrics.append(t_statistic)
                group_labels.append(group_name)

    values = np.array(balance_metrics)

    return GroupedMetric(values, group_labels, name=&#39;Balance Test&#39;)</code></pre>
</details>
</dd>
<dt id="QIDLearningLib.metrics.causality.covariate_shift"><code class="name flex">
<span>def <span class="ident">covariate_shift</span></span>(<span>df, quasi_identifiers, treatment_col, treatment_value)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate covariate shift metric for a given DataFrame and attributes.</p>
<p>Synopse:
Covariate shift metric measures the difference in distribution between treated and control groups based on specified attributes.</p>
<p>Details:
The metric is computed by comparing the distribution of attribute values between treated and control groups using the Kolmogorov-Smirnov statistic. Additionally, it considers the overall distribution difference across the entire DataFrame.</p>
<p>Parameters:
- df (pd.DataFrame): The input DataFrame.
- quasi_identifiers (list): List of column names representing the quasi-identifiers.
- treatment_col (str): The column name representing the treatment indicator.
- treatment_value: The value indicating the treated group.</p>
<p>Return:
GroupedMetric: An object containing covariate shift metric values and corresponding group labels.</p>
<p>Example:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 1, 2], 'B': ['X', 'Y', 'X', 'Y'], 'Treatment': [1, 0, 1, 0]})
&gt;&gt;&gt; metric_result = covariate_shift(df, ['A', 'B'], 'Treatment', 1)
&gt;&gt;&gt; print(metric_result.values)
[0.1, 0.2, ...]  # Covariate shift metric values
&gt;&gt;&gt; print(metric_result.group_labels)
['Group 1', 'Group 2', ...]  # Corresponding group labels
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def covariate_shift(df, quasi_identifiers, treatment_col, treatment_value):
    &#34;&#34;&#34;
    Calculate covariate shift metric for a given DataFrame and attributes.

    Synopse:
    Covariate shift metric measures the difference in distribution between treated and control groups based on specified attributes.

    Details:
    The metric is computed by comparing the distribution of attribute values between treated and control groups using the Kolmogorov-Smirnov statistic. Additionally, it considers the overall distribution difference across the entire DataFrame.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - quasi_identifiers (list): List of column names representing the quasi-identifiers.
    - treatment_col (str): The column name representing the treatment indicator.
    - treatment_value: The value indicating the treated group.

    Return:
    GroupedMetric: An object containing covariate shift metric values and corresponding group labels.

    Example:
    &gt;&gt;&gt; df = pd.DataFrame({&#39;A&#39;: [1, 2, 1, 2], &#39;B&#39;: [&#39;X&#39;, &#39;Y&#39;, &#39;X&#39;, &#39;Y&#39;], &#39;Treatment&#39;: [1, 0, 1, 0]})
    &gt;&gt;&gt; metric_result = covariate_shift(df, [&#39;A&#39;, &#39;B&#39;], &#39;Treatment&#39;, 1)
    &gt;&gt;&gt; print(metric_result.values)
    [0.1, 0.2, ...]  # Covariate shift metric values
    &gt;&gt;&gt; print(metric_result.group_labels)
    [&#39;Group 1&#39;, &#39;Group 2&#39;, ...]  # Corresponding group labels

    &#34;&#34;&#34;
    treated = df[df[treatment_col] == treatment_value]
    control = df[df[treatment_col] != treatment_value]

    overall_distribution = df[quasi_identifiers].value_counts(normalize=True)

    grouped_treated = treated.groupby(quasi_identifiers)
    grouped_control = control.groupby(quasi_identifiers)

    shift_metrics = []
    group_labels = []

    # Flatten data and encode categorical columns
    df_flat_encoded = encode_categorical(df[quasi_identifiers].reset_index(drop=True), quasi_identifiers)

    for group_name, group_df_treated in grouped_treated:
        # Check if the control group also has data for the same group_name
        if group_name in grouped_control.groups:
            group_df_control = grouped_control.get_group(group_name)

            # Flatten data and encode categorical columns in treated and control groups
            group_df_treated_flat_encoded = encode_categorical(group_df_treated.reset_index(drop=True), quasi_identifiers)
            group_df_control_flat_encoded = encode_categorical(group_df_control.reset_index(drop=True), quasi_identifiers)

            # Check if the groups have data
            if not group_df_treated_flat_encoded.empty and not group_df_control_flat_encoded.empty:
                # Calculate overall distribution difference
                overall_diff = np.sum(
                    np.abs(overall_distribution - group_df_treated_flat_encoded.value_counts(normalize=True, sort=False)))

                # Calculate covariate shift metric
                ks_statistic, _ = ks_test(
                    df_flat_encoded.loc[group_df_treated_flat_encoded.index].values.flatten(),
                    df_flat_encoded.loc[group_df_control_flat_encoded.index].values.flatten()
                )

                shift_metrics.append(ks_statistic + overall_diff)
                group_labels.append(group_name)

    values = np.array(shift_metrics)
    return GroupedMetric(values, group_labels, name=&#39;Covariate Shift&#39;)</code></pre>
</details>
</dd>
<dt id="QIDLearningLib.metrics.causality.propensity_score_overlap"><code class="name flex">
<span>def <span class="ident">propensity_score_overlap</span></span>(<span>df, quasi_identifiers, treatment_col, treatment_value)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate propensity score overlap metric for a given DataFrame and attributes.</p>
<p>Synopse:
Propensity score overlap metric measures the degree of overlap in propensity scores between treated and control groups based on specified quasi-identifiers.</p>
<p>Details:
The metric is computed by fitting a logistic regression model to predict the treatment indicator based on quasi-identifiers. Propensity scores are then extracted, and the mean overlap is calculated.</p>
<p>Parameters:
- df (pd.DataFrame): The input DataFrame.
- quasi_identifiers (list): List of column names representing the quasi-identifiers.
- treatment_col (str): The column name representing the treatment indicator.
- treatment_value: The value indicating the treated group.</p>
<p>Return:
GroupedMetric: An object containing propensity score overlap metric values and corresponding group labels.</p>
<p>Example:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 1, 2], 'B': ['X', 'Y', 'X', 'Y'], 'Treatment': [1, 0, 1, 0]})
&gt;&gt;&gt; metric_result = propensity_score_overlap(df, ['A', 'B'], 'Treatment', 1)
&gt;&gt;&gt; print(metric_result.values)
[0.1, 0.2, ...]  # Propensity score overlap metric values
&gt;&gt;&gt; print(metric_result.group_labels)
['Treatment 1', 'Treatment 2', ...]  # Corresponding group labels
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def propensity_score_overlap(df, quasi_identifiers, treatment_col, treatment_value):
    &#34;&#34;&#34;
    Calculate propensity score overlap metric for a given DataFrame and attributes.

    Synopse:
    Propensity score overlap metric measures the degree of overlap in propensity scores between treated and control groups based on specified quasi-identifiers.

    Details:
    The metric is computed by fitting a logistic regression model to predict the treatment indicator based on quasi-identifiers. Propensity scores are then extracted, and the mean overlap is calculated.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - quasi_identifiers (list): List of column names representing the quasi-identifiers.
    - treatment_col (str): The column name representing the treatment indicator.
    - treatment_value: The value indicating the treated group.

    Return:
    GroupedMetric: An object containing propensity score overlap metric values and corresponding group labels.

    Example:
    &gt;&gt;&gt; df = pd.DataFrame({&#39;A&#39;: [1, 2, 1, 2], &#39;B&#39;: [&#39;X&#39;, &#39;Y&#39;, &#39;X&#39;, &#39;Y&#39;], &#39;Treatment&#39;: [1, 0, 1, 0]})
    &gt;&gt;&gt; metric_result = propensity_score_overlap(df, [&#39;A&#39;, &#39;B&#39;], &#39;Treatment&#39;, 1)
    &gt;&gt;&gt; print(metric_result.values)
    [0.1, 0.2, ...]  # Propensity score overlap metric values
    &gt;&gt;&gt; print(metric_result.group_labels)
    [&#39;Treatment 1&#39;, &#39;Treatment 2&#39;, ...]  # Corresponding group labels

    &#34;&#34;&#34;
    X = df[quasi_identifiers]
    y = df[treatment_col]

    # Identify categorical columns
    categorical_cols = X.select_dtypes(include=[&#39;object&#39;]).columns
    numerical_cols = X.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns

    # Create a ColumnTransformer to apply one-hot encoding to categorical columns
    preprocessor = ColumnTransformer(
        transformers=[
            (&#39;cat&#39;, OneHotEncoder(), categorical_cols),
            (&#39;num&#39;, &#39;passthrough&#39;, numerical_cols)
        ],
        remainder=&#39;drop&#39;
    )

    # Create a pipeline with the preprocessor and logistic regression model
    model = Pipeline(steps=[
        (&#39;preprocessor&#39;, preprocessor),
        (&#39;classifier&#39;, LogisticRegression())
    ])

    # Fit the entire pipeline
    model.fit(X, y)

    # Transform the data
    X_encoded = model.named_steps[&#39;preprocessor&#39;].transform(X)

    # Extract propensity scores for all treated samples against all control samples
    propensity_scores_treated = model.named_steps[&#39;classifier&#39;].predict_proba(X_encoded[y == treatment_value])[:, 1]
    propensity_scores_control = model.named_steps[&#39;classifier&#39;].predict_proba(X_encoded[y != treatment_value])[:, 1]

    # Calculate the mean propensity score overlap for each treated sample
    overlap_metrics = []

    for treated_score in propensity_scores_treated:
        overlap_metric = np.mean(np.abs(treated_score - propensity_scores_control))
        overlap_metrics.append(overlap_metric)

    # Use treatment labels as group labels
    group_labels = [f&#39;Treatment {i+1}&#39; for i in range(len(overlap_metrics))]

    values = np.array(overlap_metrics)

    return GroupedMetric(values, group_labels=group_labels, name=&#39;Propensity Score Overlap&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="QIDLearningLib.metrics" href="index.html">QIDLearningLib.metrics</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="QIDLearningLib.metrics.causality.balance_test" href="#QIDLearningLib.metrics.causality.balance_test">balance_test</a></code></li>
<li><code><a title="QIDLearningLib.metrics.causality.covariate_shift" href="#QIDLearningLib.metrics.causality.covariate_shift">covariate_shift</a></code></li>
<li><code><a title="QIDLearningLib.metrics.causality.propensity_score_overlap" href="#QIDLearningLib.metrics.causality.propensity_score_overlap">propensity_score_overlap</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>